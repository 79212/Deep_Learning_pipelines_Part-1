{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://cocl.us/pytorch_link_top\">\n",
    "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n",
    "</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Preprocessing and Gradient Descent </h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "<p>This lab will show how data normalization, data standardization, data decorrelation (Principal Component Analysis), Whitening Data and Zero-Phase Component Analysis affect convergence in parameter space. The simulations are based on the paper Efficient BackProp by Yann A. LeCun1, Léon Bottou1, Genevieve B. Orr2, and Klaus-Robert Müller.  </p>\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#Auxiliary\">Auxiliary Functions and Classes </a></li>\n",
    "    <li><a href=\"#PyTorch_Classes\"> Define the PyTorch Classes </a></li>\n",
    "    <li><a href=\"#No_Transform\">Data with No Pre-processing </a></li>\n",
    "    <li><a href=\"#Standardize_Data\">Standardize Data </a></li>\n",
    "    <li><a href=\"#PCA\">PCA </a></li>\n",
    "    <li><a href=\"#Whitening\">Whitening</a></li>\n",
    "    <li><a href=\"#ZCA\">Zero-Phase Component Analysis</a></li>\n",
    "</ul>\n",
    "\n",
    "<p>Estimated Time Needed: <strong>30 min</strong></p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Auxiliary\">Auxiliary Functions and Classes </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need the following libraries for ploting:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the libraries we are going to use in the lab.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class <code>plot_error_surfaces</code> is just to help you visualize the data space and the parameter space during training and has nothing to do with PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for ploting  \n",
    "\n",
    "class plot_error_surfaces(object):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, w_range, b_range, X, Y, n_samples = 40, go = True):\n",
    "        W = np.linspace(-w_range, w_range, n_samples)\n",
    "        B = np.linspace(-b_range, b_range, n_samples)\n",
    "        w, b = np.meshgrid(W, B)    \n",
    "        Z = np.zeros((n_samples, n_samples))\n",
    "        count1 = 0\n",
    "        self.y = Y.numpy()\n",
    "        self.x = X.numpy()\n",
    "        for w1, b1 in zip(w, b):\n",
    "            count2 = 0\n",
    "            for w2, b2 in zip(w1, b1):\n",
    "                Z[count1, count2] = np.mean((self.y - w2 * self.x[:,0] - b2*self.x[:,1]) ** 2)\n",
    "                count2 += 1\n",
    "            count1 += 1\n",
    "        self.Z = Z\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.W = []\n",
    "        self.B = []\n",
    "        self.LOSS = []\n",
    "        self.n = 0\n",
    "        if go == True:\n",
    "            plt.figure()\n",
    "            plt.figure(figsize = (7.5, 5))\n",
    "            plt.axes(projection = '3d').plot_surface(self.w, self.b, self.Z, rstride = 1, cstride = 1, cmap = 'viridis', edgecolor = 'none')\n",
    "            plt.title('Loss Surface')\n",
    "            plt.xlabel('w')\n",
    "            plt.ylabel('b')\n",
    "            plt.show()\n",
    "            plt.figure()\n",
    "            plt.title('Loss Surface Contour')\n",
    "            plt.xlabel('w_{1}')\n",
    "            plt.ylabel('w_{2}')\n",
    "            plt.contour(self.w, self.b, self.Z)\n",
    "            plt.show()\n",
    "            \n",
    "    # Setter\n",
    "    def set_para_loss(self, model, loss):\n",
    "        self.n = self.n + 1\n",
    "        self.LOSS.append(loss)\n",
    "        self.W.append(model.state_dict()['linear.weight'][0][0].item())\n",
    "        self.B.append(model.state_dict()['linear.weight'][0][1].item())\n",
    "    \n",
    "    # Plot diagram\n",
    "    def final_plot(self): \n",
    "        ax = plt.axes(projection = '3d')\n",
    "        ax.plot_wireframe(self.w, self.b, self.Z)\n",
    "        ax.scatter( self.W,self.B,self.LOSS, c = 'r', marker = 'x', s = 200, alpha = 1)\n",
    "        plt.figure()\n",
    "        plt.contour( self.w,self.b, self.Z)\n",
    "        plt.scatter(self.W,self.B,   c = 'r', marker = 'x')\n",
    "        for i in range(len(self.W)):\n",
    "            plt.annotate(str(i), ( self.W[i],self.B[i]))\n",
    "        plt.xlabel('w_{1}')\n",
    "        plt.ylabel('w_{2}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will plot out a scatter plot of several datasets and their covariance matrix; the input is a dictionary, the key is the name of the dataset, and the value is a   PyTorch dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def plotData(datasets):   \n",
    "    for name,dataset in datasets.items():\n",
    "        plt.scatter(dataset.X.numpy()[:,0], dataset.X.numpy()[:,1],label=name)\n",
    "        plt.legend()\n",
    "      \n",
    "    plt.show()    \n",
    "    for name,dataset in datasets.items():    \n",
    "        df = pd.DataFrame(dataset.X.numpy())\n",
    "        corr = df.corr()\n",
    "        sns.heatmap(corr,annot=corr)\n",
    "        plt.title('correlation of {} data'.format(name))\n",
    "        \n",
    "        plt.show('correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"PyTorch_Classes\">Define the PyTorch Classes</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import PyTroch libraries and set random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and set random seed\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset object; this object has the option to apply the appropriate transform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Class\n",
    "\n",
    "class Data(Dataset):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self,range_x1=2,range_x2=10,samples=100,standardiz=False,PCA=False,whitening=False,zero_phase=False):\n",
    "\n",
    "        self.W=torch.tensor([[-1.0],[1.0]])\n",
    "        self.X=torch.zeros(samples,2)\n",
    "        self.X[:,0] = torch.linspace(start = -1*range_x1, end= range_x1, steps =samples)\n",
    "        self.X[:,1] = torch.linspace(start = -1*range_x2, end= range_x2, steps =samples)\n",
    "        self.X=self.X+torch.randn(samples,2)*1\n",
    "        self.Y = torch.mm(self.X,self.W)+torch.randn(samples,1)\n",
    "        if standardiz==True:\n",
    "            self.X=torch.mm(self.X-self.X.mean(dim=0),torch.eye(2)/self.X.std(dim=0)) \n",
    "        if PCA==True:\n",
    "            self.X=self.X-self.X.mean(dim=0)\n",
    "            self.Cov=torch.mm(torch.t(self.X),self.X)/self.X.shape[0]\n",
    "            self.eigenvalues,self.eigenvectors=torch.eig(self.Cov,True)\n",
    "            self.X=torch.mm(self.X,self.eigenvectors)\n",
    "            \n",
    "        if whitening==True:\n",
    "            self.X=self.X-self.X.mean(dim=0)\n",
    "            self.Cov=torch.mm(torch.t(self.X),self.X)/self.X.shape[0]\n",
    "            self.eigenvalues,self.eigenvectors=torch.eig(self.Cov,True)\n",
    "            self.diag=torch.eye(2)\n",
    "            self.diag[0,0]=self.eigenvalues[0,0]**(-1/2)\n",
    "            self.diag[1,1]=self.eigenvalues[1,0]**(-1/2)\n",
    "            self.whitening_transform=torch.mm(self.eigenvectors,self.diag)\n",
    "            self.X=torch.mm(self.X,self.whitening_transform)\n",
    "            \n",
    "            \n",
    "        if zero_phase==True:\n",
    "            self.X=self.X-self.X.mean(dim=0)\n",
    "            self.Cov=torch.mm(torch.t(self.X),self.X)/self.X.shape[0]\n",
    "            self.eigenvalues,self.eigenvectors=torch.eig(self.Cov,True)\n",
    "            self.diag=torch.eye(2)\n",
    "            self.diag[0,0]=self.eigenvalues[0,0]**(-1/2)\n",
    "            self.diag[1,1]=self.eigenvalues[1,0]**(-1/2)\n",
    "            self.whitening_transform=torch.mm(self.eigenvectors,self.diag)\n",
    "            self.X=torch.mm(self.X,self.whitening_transform)\n",
    "            self.X=torch.mm(self.X,torch.t(self.eigenvectors))\n",
    "        self.len=samples\n",
    "        \n",
    "        \n",
    "    # Getter\n",
    "    def __getitem__(self,index):    \n",
    "        return self.X[index],self.Y[index]\n",
    "    \n",
    "    # Get Length\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "class linear_regression(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(linear_regression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        yhat = self.linear(x)\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Parameters and dictionary to store loss for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss={'regular':[],'standardize':[],'pca':[],'whitening':[],'zca':[]}\n",
    "#learning rate\n",
    "lr=0.01 \n",
    "batch_size=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#No_Transform\">  Data with No Pre-processing  </h2>\n",
    "  In this section, we create data with no Preprocessing , if  $\\mathbf{\\hat{x}}$ is a transformed feature vector  and $\\mathbf{x}$ in this case our data is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\mathbf{\\hat{x}}=\\mathbf{x}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object\n",
    "dataset = Data(range_x1=2,range_x2=10)\n",
    "dataset_standardize = Data(range_x1=2,range_x2=10,standardiz=True)\n",
    "dataset_pca = Data(range_x1=2,range_x2=10,PCA=True)\n",
    "dataset_whitening= Data(range_x1=2,range_x2=10,whitening=True)\n",
    "dataset_zca=Data(range_x1=2,range_x2=10,zero_phase=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot data with no Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets={'regular dataset':dataset}        \n",
    "plotData(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a linear regression object, and we initialize the values, so they are relatively far away from the minimum. We also create an optimizer object and a data loader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build in cost function\n",
    "model=linear_regression(2,1)\n",
    "start_w1=-15.0\n",
    "start_w2=20.0\n",
    "model.state_dict()['linear.weight'][0][0] = start_w1\n",
    "model.state_dict()['linear.weight'][0][1] = start_w2\n",
    "model.state_dict()['linear.bias'][0]=0.0\n",
    "trainloader = DataLoader(dataset = dataset, batch_size = 1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a plotting object, not part of PyTroch, just used to help visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_surface = plot_error_surfaces(20,20, dataset.X, dataset.Y, 30, go = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 10 epochs of stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    for x,y in trainloader:\n",
    "        yhat = model(x)\n",
    "        loss = criterion(yhat, y)\n",
    "        training_loss['regular'].append(loss.item())\n",
    "        get_surface.set_para_loss(model, loss.tolist())          \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss surface and overlay the values of the parameters obtained via gradient descent in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_surface.final_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#Standardize_Data \"> Standardize Data </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we Standardize data $\\mathbf{x}$, this is equivalent to the following matrix operation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\quad\n",
    "    \\boldsymbol D= \\begin{pmatrix} \\sigma_1 & 0 \\\\\n",
    "                             0  & \\sigma_2 \\end{pmatrix}  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{\\hat{x}}=D^{-1}(\\mathbf x-\\boldsymbol\\mu)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\boldsymbol\\mu$ is the mean and $\\sigma_i$ is the standard deviation of the i-th component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataset object where the data has been standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_standardize = Data(range_x1=2,range_x2=10,standardiz=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets={'regular dataset':dataset, 'standardized datatset': dataset_standardize}        \n",
    "        \n",
    "plotData(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a linear regression object, and we initialize the values, so they are relatively far away from the minimum. We also create an optimizer object and a data loader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_standardize=linear_regression(2,1)\n",
    "trainloader = DataLoader(dataset = dataset_standardize, batch_size = 1)\n",
    "model_standardize.state_dict()['linear.weight'][0][0] = start_w1\n",
    "model_standardize.state_dict()['linear.weight'][0][1] = start_w2\n",
    "model_standardize.state_dict()['linear.bias'][0]=0.0\n",
    "optimizer = optim.SGD(model_standardize.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the loss surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_surface = plot_error_surfaces(20,20, dataset_standardize.X, dataset_standardize.Y, 30, go = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 10 epochs of stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    for x,y in trainloader:\n",
    "        yhat = model_standardize(x)\n",
    "        loss = criterion(yhat, y)\n",
    "        training_loss['standardize'].append(loss.item())\n",
    "        get_surface.set_para_loss(model_standardize, loss.tolist())          \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss surface and overlay the values of the parameters obtained via gradient descent in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_surface.final_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#PCA \"> PCA</h2>\n",
    "In this section, we create a dataset object that uses Principal component analysis (PCA). We find the projection of the data on the eigenvectors of the covariance matrix $\\mathbf{Q}$, as shown below. We zero center the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{1}{N}   \\mathbf{X}^T \\mathbf{X} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^T$\n",
    "\n",
    "$\\mathbf{\\hat{x}}=\\mathbf{x} \\mathbf{Q} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we crate the dataset object :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pca = Data(range_x1=2,range_x2=10,PCA=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datasets={'regular dataset':dataset, 'PCA': dataset_pca }        \n",
    "        \n",
    "plotData(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a linear regression object, and we initialize the values, so they are relatively far away from the minimum. We also create an optimizer object and a data loader object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pca=linear_regression(2,1)\n",
    "trainloader = DataLoader(dataset = dataset_pca, batch_size = 1)\n",
    "model_pca.state_dict()['linear.weight'][0][0] = start_w1\n",
    "model_pca.state_dict()['linear.weight'][0][1] = start_w2\n",
    "model_pca.state_dict()['linear.bias'][0]=0.0\n",
    "optimizer = optim.SGD(model_pca.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we plat the loss surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_surface = plot_error_surfaces(20,20, dataset_pca.X, dataset_pca.Y, 30, go = True)\n",
    "print(\"standard deviation\", dataset_pca.X.std(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 10 epochs of stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    for x,y in trainloader:\n",
    "        yhat = model_pca(x)\n",
    "        loss = criterion(yhat, y)\n",
    "        training_loss['pca'].append(loss.item())\n",
    "        get_surface.set_para_loss(model_pca, loss.tolist())          \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss surface and overlay the values of the parameters obtained via gradient descent in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_surface.final_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#Whitening<\"> Whitening</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we apply a Whitening Matrix, this gives the features all the same variance. The operation can be expressed as: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{\\hat{x}}=\\mathbf{x} \\mathbf{Q} \\mathbf{\\Lambda}^{-1/2} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_whitening= Data(range_x1=2,range_x2=10,whitening=True)\n",
    "\n",
    "datasets={'hitening dataset':dataset_whitening, 'PCA': dataset_pca}        \n",
    "        \n",
    "plotData(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a linear regression object, and we initialize the values, so they are relatively far away from the minimum. We also create an optimizer object and a data loader object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_whitening=linear_regression(2,1)\n",
    "criterion = nn.MSELoss()\n",
    "trainloader = DataLoader(dataset = dataset_whitening, batch_size = 1)\n",
    "model_whitening.state_dict()['linear.weight'][0][0] = start_w1\n",
    "model_whitening.state_dict()['linear.weight'][0][1] = start_w2\n",
    "model_whitening.state_dict()['linear.bias'][0]=0.0\n",
    "optimizer = optim.SGD(model_whitening.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we plot the loss surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_surface = plot_error_surfaces(20,20, dataset_whitening.X, dataset_whitening.Y, 30, go = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 10 epochs of stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    for x,y in trainloader:\n",
    "        yhat = model_whitening(x)\n",
    "        loss = criterion(yhat, y)\n",
    "        training_loss['whitening'].append(loss.item())\n",
    "        get_surface.set_para_loss(model_whitening, loss.tolist())          \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss surface and overlay the values of the parameters obtained via gradient descent in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_surface.final_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#ZCA\"> Zero-Phase Component Analysis (ZCA) </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply ZCA, ZCA is decorrelated and has Whitening applied to it, but the data has more income with the original data. We ca apply the transform the data as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{\\hat{x}}=\\mathbf{x} \\mathbf{Q} \\mathbf{\\Lambda}^{-1/2}\\mathbf{Q}^{T} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create the data and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_zca=Data(range_x1=2,range_x2=10,zero_phase=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets={'ZCA':dataset_zca, 'whitening': dataset_whitening }  \n",
    "plotData(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a linear regression object, and we initialize the values, so they are relatively far away from the minimum. We also create an optimizer object and a data loader object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_zca=linear_regression(2,1)\n",
    "criterion = nn.MSELoss()\n",
    "trainloader = DataLoader(dataset = dataset_zca, batch_size = 1)\n",
    "model_zca.state_dict()['linear.weight'][0][0] = start_w1\n",
    "model_zca.state_dict()['linear.weight'][0][1] = start_w2\n",
    "model_zca.state_dict()['linear.bias'][0]=0.0\n",
    "optimizer = optim.SGD(model_zca.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we plot the loss surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_surface = plot_error_surfaces(20,20, dataset_zca.X, dataset_zca.Y, 30, go = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 10 epochs of stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    for x,y in trainloader:\n",
    "        yhat = model_zca(x)\n",
    "        loss = criterion(yhat, y)\n",
    "        training_loss['zca'].append(loss.item())\n",
    "        get_surface.set_para_loss(model_zca, loss.tolist())          \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss surface and overlay the values of the parameters obtained via gradient descent in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_surface.final_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#loss\"> Plot Loss   </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the loss for each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, loss in training_loss.items():\n",
    "    \n",
    "        plt.plot(loss,label=name )\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>About the Authors:</h2> \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2020 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
