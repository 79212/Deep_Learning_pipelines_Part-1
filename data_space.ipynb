{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://cocl.us/pytorch_link_top\">\n",
    "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n",
    "</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Preprocessing and Gradient Descent </h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "<p>This lab will show how data normalization, data standardization, data decorrelation (Principal Component Analysis), Whitening Data and Zero-Phase Component Analysis affect convergence in parameter space. The simulations are based on the paper Efficient BackProp by Yann A. LeCun1, Léon Bottou1, Genevieve B. Orr2, and Klaus-Robert Müller.  </p>\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#Auxiliary\">Auxiliary Functions and Classes </a></li>\n",
    "    <li><a href=\"#PyTorch_Classes\"> Define the PyTorch Classes </a></li>\n",
    "    <li><a href=\"#No_Transform\">Data with No Pre-processing </a></li>\n",
    "    <li><a href=\"#Standardize_Data\">Standardize Data </a></li>\n",
    "    <li><a href=\"#PCA\">PCA </a></li>\n",
    "    <li><a href=\"#Whitening\">Whitening</a></li>\n",
    "    <li><a href=\"#ZCA\">Zero-Phase Component Analysis</a></li>\n",
    "    <li><a href=\"#WHYZCA\">Why ZCA?</a></li>\n",
    "</ul>\n",
    "\n",
    "<p>Estimated Time Needed: <strong>30 min</strong></p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Auxiliary\">DataSet </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need the following libraries for ploting:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the libraries we are going to use in the lab.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate 2D  data that is correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=200\n",
    "\n",
    "u=torch.tensor([[1.0,1.0],[0.10,-0.10]])/(2)**(0.5)\n",
    "\n",
    "X=torch.mm(4*torch.randn(samples,2),u)+2\n",
    "plt.scatter(X[:, 0].numpy(), X[:, 1].numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#Standardize_Data \"> Standardize Data </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we Standardize data $\\mathbf{x}$, this is equivalent to the following matrix operation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   $\\quad\n",
    "    \\boldsymbol D= \\begin{pmatrix} \\sigma_1 & 0 \\\\\n",
    "                             0  & \\sigma_2 \\end{pmatrix}  $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{\\hat{x}}=(\\mathbf x-\\boldsymbol\\mu)D^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\boldsymbol\\mu$ is the mean and $\\sigma_i$ is the standard deviation of the i-th component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xhat=torch.mm(X-X.mean(dim=0),torch.eye(2)/X.std(dim=0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0].numpy(), X[:, 1].numpy(),label=\"No Pre-processing\")\n",
    "plt.scatter(Xhat[:, 0].numpy(), Xhat[:, 1].numpy(),label=\"Standardize Data\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on we will deal with zero zero centered data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf x=\\mathbf x-\\boldsymbol\\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X-X.mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0].numpy(), X[:, 1].numpy(),label=\"No Pre-processing\")\n",
    "plt.scatter(Xhat[:, 0].numpy(), Xhat[:, 1].numpy(),label=\"zero-mean\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#PCA \"> PCA</h2>\n",
    "In this section, we create a dataset object that uses Principal component analysis (PCA). We find the projection of the data on the eigenvectors of the covariance matrix $\\mathbf{Q}$, as shown below. We zero center the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{1}{N}   \\mathbf{X}^T \\mathbf{X} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^T$\n",
    "\n",
    "$\\mathbf{\\hat{x}}=\\mathbf{x} \\mathbf{Q} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the empirical covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{1}{N}   \\mathbf{X}^T \\mathbf{X}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cov=torch.mm(torch.t(X),X)/X.shape[0]\n",
    "Cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the eigenvectors\n",
    "$\\frac{1}{N}   \\mathbf{X}^T \\mathbf{X} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues,eigenvectors=torch.eig(Cov,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can plot the eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_vec=torch.t(eigenvectors).numpy()\n",
    "plt.scatter(X[:, 0].numpy(), X[:, 1].numpy(),label=\"data\")\n",
    "plt.quiver([0],[0],row_vec[:,0],row_vec[:,1],label=\"Eigen vectors\")\n",
    "plt.xlabel(\"x_{1}\")\n",
    "plt.ylabel(\"x_{2}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the projection  the eigenvectors:\n",
    "$\\mathbf{\\hat{x}}=\\mathbf{x} \\mathbf{Q} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xhat=torch.mm(X,eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0].numpy(), X[:, 1].numpy(),label=\"data\") \n",
    "\n",
    "plt.scatter(Xhat[:, 0].numpy(), Xhat[:, 1].numpy(),label=\"transformed data\")\n",
    "plt.xlabel(\"q_{1}\")\n",
    "plt.ylabel(\"q_{2}\")\n",
    "plt.quiver([0],[0],row_vec[:,0],row_vec[:,1],label=\"Eigen vectors\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see the data is now uncorrelated: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mm(torch.t(Xhat),Xhat)/Xhat.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but the data has a  standard deviation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xhat.std(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#Whitening<\"> Whitening</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we apply a Whitening Matrix, this gives the features all the same variance. The operation can be expressed as: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{\\hat{x}}=\\mathbf{x} \\mathbf{Q} \\mathbf{\\Lambda}^{-1/2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the same process as PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cov=torch.mm(torch.t(X),X)/X.shape[0]\n",
    "eigenvalues,eigenvectors=torch.eig(Cov,True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the diagonal matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag=torch.eye(2)\n",
    "diag[0,0]=eigenvalues[0,0]**(-1/2)\n",
    "diag[1,1]=eigenvalues[1,0]**(-1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xhat=torch.mm(torch.mm(X,eigenvectors),diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a linear regression object, and we initialize the values, so they are relatively far away from the minimum. We also create an optimizer object and a data loader object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0].numpy(), X[:, 1].numpy(),label=\"data\") \n",
    "plt.scatter(Xhat[:, 0].numpy(), Xhat[:, 1].numpy(),label=\"transformed data\")\n",
    "plt.xlabel(\"q_{1}\")\n",
    "plt.ylabel(\"q_{2}\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see the Standard deviation of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xhat.std(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#ZCA\"> Zero-Phase Component Analysis (ZCA) </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply ZCA, ZCA is decorrelated and has Whitening applied to it, but the data has more income with the original data. We ca apply the transform the data as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{\\hat{x}}=\\mathbf{x} \\mathbf{Q} \\mathbf{\\Lambda}^{-1/2}\\mathbf{Q}^{T} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply Whitening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cov=torch.mm(torch.t(X),X)/X.shape[0]\n",
    "eigenvalues,eigenvectors=torch.eig(Cov,True)\n",
    "diag=torch.eye(2)\n",
    "diag[0,0]=eigenvalues[0,0]**(-1/2)\n",
    "diag[1,1]=eigenvalues[1,0]**(-1/2)\n",
    "Xhat=torch.mm(torch.mm(X,eigenvectors),diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then find the projection back into space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xhat=torch.mm(Xhat,torch.t(eigenvectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0].numpy(), X[:, 1].numpy(),label=\"data\") \n",
    "plt.scatter(Xhat[:, 0].numpy(), Xhat[:, 1].numpy(),label=\"ZCA\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#loss\"> Why ZCA   </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to PCA, ZCA has preserved the orientation of the original data points, this import in many applications. Let create some data and label it to persevere the orientation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=200\n",
    "\n",
    "W1=3*torch.tensor([[1.0,1.0],[0.10,-0.10]])/(2)**(0.5)\n",
    "W2=torch.tensor([[0.1,0.1],[1,-1]])/(2)**(0.5)\n",
    "data_set1=torch.mm(torch.randn(samples,2),W1)\n",
    "data_set2=torch.mm(torch.randn(samples,2),W2)\n",
    "plt.scatter(data_set1[:, 0].numpy(), data_set1[:, 1].numpy(),c='r')\n",
    "plt.scatter(data_set2[:, 0].numpy(), data_set2[:, 1].numpy(),c='b')\n",
    "plt.xlabel(\"x_{1}\")\n",
    "plt.ylabel(\"x_{2}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the data into one dataset to calculate PCA and ZCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= torch.cat((data_set1, data_set2), 0)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate PCA and find the project onto the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cov=torch.mm(torch.t(X),X)/X.shape[0]\n",
    "eigenvalues,eigenvectors=torch.eig(Cov,True)\n",
    "data_one_new=torch.mm(data_set1,eigenvectors)\n",
    "data_two_new=torch.mm(data_set2,eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the PCA and the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2)\n",
    "axs[0].scatter(data_set1[:, 0].numpy(), data_set1[:, 1].numpy(),c='r')\n",
    "axs[0].scatter(data_set2[:, 0].numpy(), data_set2[:, 1].numpy(),c='b')\n",
    "axs[0].title.set_text(\"DATA\")\n",
    "axs[1].scatter(data_one_new[:, 0].numpy(), data_one_new[:, 1].numpy(),c='r')\n",
    "axs[1].scatter(data_two_new[:, 0].numpy(), data_two_new[:, 1].numpy(),c='b')\n",
    "axs[1].title.set_text(\"PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the orientation of the data appears different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag[0,0]=eigenvalues[0,0]**(-1/2)\n",
    "diag[1,1]=eigenvalues[1,0]**(-1/2)\n",
    "transform=torch.mm(torch.mm(eigenvectors,diag),torch.t(eigenvectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can apply the ZCA transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_one_new=torch.mm(data_set1,transform)\n",
    "data_two_new=torch.mm(data_set2,transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can plot the data we see that ZCA preserves the transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2)\n",
    "axs[0].scatter(data_set1[:, 0].numpy(), data_set1[:, 1].numpy(),c='r')\n",
    "axs[0].scatter(data_set2[:, 0].numpy(), data_set2[:, 1].numpy(),c='b')\n",
    "axs[0].title.set_text(\"DATA\")\n",
    "axs[1].scatter(data_one_new[:, 0].numpy(), data_one_new[:, 1].numpy(),c='r')\n",
    "axs[1].scatter(data_two_new[:, 0].numpy(), data_two_new[:, 1].numpy(),c='b')\n",
    "axs[1].title.set_text(\"PZCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>About the Authors:</h2> \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2020 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
